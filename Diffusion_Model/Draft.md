我是不是可以这么理解。首先我们会有一团学习器parameter（当然一般是一个神经网络），我们称为P。 

 之后我们每一组放入模型的数据由以下构成： 

+ t（一般是一个整数，告诉模型现在在哪个步骤了）
+ Xt (t时刻的图像) 

 模型要做的事情就是基于t和Xt来预测t step中我们到底添加了什么高斯噪声





一开始有一个疑问

> 首先，模型P所做的事情是如何对一个图形进行去噪。而这个噪声一开始是由人通过某种规则添加的对吧？本质我们是让模型学习人添加噪声的规律？这真的有意义吗，因为我们不需要用模型去学习一个我们已知的东西



但是实际上我们学习的是低信图形与真实图像的差值（某种程度上是一种有效学习真实图像分布的有效训练范式）





或者说模型P是预测t时刻的噪声图，它是一个size等于原图，同时也存在RBG通道的图。（所以本质diffusion提出了一个通过学习噪声图进而来学习真实世界（给定）图像分布的训练范式









> 说来很神奇，当我在思考diffusion model是如何从一张高噪声（或者说几乎没有任何有用信息的黑白噪点图）来一步步进行去噪的时候，我联想到了LLM
>
> 
>
> LLM在某些时候也有类似的处境，假设context很简略“北京是”，后面能跟太多东西了，比如是中国的首都，比如是北方最大的城市，比如是五朝古都。。。
>
> 
>
> 这种情况下的训练真的是合理的吗。因为我们对Model的定义是一对一的Mapping？
>
> 或者说我们认为模型的预测是需要基于一定context的，这种过少context的预测是否会对模型产生不良的影响





常规的一个应用方法是，随机初始化一个噪声，铺满一个画布，设定推理（去噪）Steps



这是不是就会导致，我们每一组训练样本，都需要一个prompt（可以是文本，也可能是其他类型），以及一个对应的图





![U-Net 的图像结果](https://th.bing.com/th/id/OIP.lvXoKMHoPJMKpKK7keZMEAHaE7?w=253&h=180&c=7&r=0&o=7&cb=12&dpr=2&pid=1.7&rm=3)